# TODO: maybe drop the general key and make these fields top level
general:
  seed: 42
  device: cuda
  target_scale: 1.0
  mate_value: 30.0 # QUEEN_SCORE * 3
  loss_function: mixed_policy
  policy_weight: [5.0, 2.0, 1.0, 0.4, 0.25, 0.125, 0.075, 0.03, 0.01, 0.005]
  mse_weight: 1.0
  epochs: 20 # With 10 epochs the model is still undertrained
  filter_threshold: null
  tags: ['test', 'fixed_mate_score', 'new_transformer_implementation', 'pv_head']

dataset:
  filename: lichess-220M-pv10-packed.bin
  oversample: False
  oversample_factor: null
  oversample_target: null
  augment_rate: 0.0
  pv_depth: 10

model:
  cnn_projection: True
  cnn_out_channels: 128
  cnn_layers: 3
  cnn_kernel_size: 3
  cnn_residual: True
  cnn_pool: False
  cnn_depthwise: False
  cnn_squeeze: True
  patch_size: 1
  dim: 128
  depth: 6
  heads: 8
  hierarchical: False
  hierarchical_blocks: 0
  stages_depth: []
  merging_strategy: 1d # TODO: test 2d mode more throughly
  mlp_dim: 256
  dropout: 0.01
  emb_dropout: 0.0
  stochastic_depth_p: 0.0
  stochastic_depth_mode: row
  random_patch_projection: False
  channel_pos_encoding: True
  learned_pos_encoding: True
  material_head: False

training:
  train_p: 0.995
  val_p: 0.0025
  test_p: 0.0025
  batch_size: 8192 # 2^13
  shuffle: True
  random_subsampling: null
  mixed_precision: True

optimizer:
  weight_decay: 1.0e-2
  # lr: base_lr * batch_size / 1024
  lr: 0.0012
  betas: [0.9, 0.999]

lr_scheduler:
  warmup_steps: 2000
  cosine_annealing: True
  cosine_tmax: 20 # same as epochs
  cosine_factor: 1
  restart: False
  min_lr: 1.0e-6

